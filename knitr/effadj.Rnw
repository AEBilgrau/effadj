\documentclass{article}

% Our preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\graphicspath{{../output/}}
\DeclareGraphicsExtensions{.eps}
\usepackage[outdir=./]{epstopdf}

\usepackage{authblk}  % For author affiliations
\usepackage{amsmath}  % Enables the align enviroment
\usepackage{amssymb}  % Math symbols (e.g. \mathbb{})
\usepackage{dsfont}   % bold fonts
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{fullpage} % Larger margins

\renewcommand{\vec}{\boldsymbol} % for bold vectors
\newcommand{\vtheta}{\vec{\theta}} % vector theta

\newcommand{\tgt}{\textrm{tgt}}
\newcommand{\rref}{\textrm{ref}}
\newcommand{\case}{\textrm{case}}
\newcommand{\ctrl}{\textrm{ctrl}}
\newcommand{\std}{\textrm{std}}
\newcommand{\Var}{\mathbb{V}\text{ar}}
\newcommand{\bbOne}{\mathds{1}}
\newcommand{\slfrac}[2]{\left.#1\middle/#2\right.}
\newcommand{\ddcq}{\Delta\Delta C_q}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Run analysis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<run_master, echo=FALSE, results='hide', message=FALSE, warning=FALSE>>=
source("../scripts/master.R")  # Simply runs the master script
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% Title of paper
\title{Unaccounted uncertainty from qPCR efficiency estimates imply uncontrolled false positive rates}

\author[12]{A.~E.~Bilgrau\thanks{To whom correspondence should be addressed (\texttt{abilgrau@math.aau.dk})}}
\author[1]{S.~Falgreen}
\author[1]{A.~Petersen}
\author[1]{M.~K.~Kjeldsen}
\author[1]{J.~S.~B{\o}dker}
\author[1]{H.~E.~Johnsen}
\author[1]{K.~Dybk{\ae}r}
\author[13]{M.~B{\o}gsted}

\affil[1]{Department of Haematology, Aalborg University Hospital}
\affil[2]{Department of Mathematical Sciences, Aalborg University}
\affil[3]{Department of Clinical Medicine, Aalborg University Hospital}

\maketitle

\begin{abstract}
\noindent\textbf{Background~~} %if any
Accurate adjustment for the amplification efficiency (AE) is an important part of real-time quantitative polymerase chain reaction (qPCR) experiments.
The most commonly used correction strategy is to estimate the AE by dilution experiments and use this as a plug-in when efficiency correcting the $\ddcq$.
Currently, it is recommended to determine the AE with high precision as this plug-in approach does not account for the AE uncertainty,
implicitly assuming an infinitely precise AE estimate.
Determining the AE with such precision, however, requires tedious laboratory work and vast amounts of biological material.
Violation of the assumption leads to overly optimistic standard errors of the $\ddcq$, confidence intervals, and $p$-values which ultimately increase the type I error rate beyond the expected significance level.
As qPCR is often used for validation it should be a high priority to account for the uncertainty of the AE estimate and thereby properly bounding the type I error rate and achieve the desired significance level.

\noindent\textbf{Results~~} %if any
We suggest and benchmark different methods to obtain the standard error of the efficiency adjusted $\ddcq$ using the statistical delta method, Monte Carlo integration, or bootstrapping.
Our suggested methods are founded in a linear mixed effects model (LMM) framework, but the problem and ideas apply in all qPCR experiments.
The methods and impact of the AE uncertainty are illustrated in three qPCR applications and a simulation study.
In addition, we validate findings suggesting that
\textit{MGST1} is differentially expressed between high and low abundance culture initiating cells in multiple myeloma
and that
microRNA-127 is differentially expressed between testicular and nodal lymphomas.

\noindent\textbf{Conclusions~~}
We conclude, that the commonly used efficiency corrected quantities disregard the uncertainty of the AE, which can drastically impact the standard error and lead to increased false positive rates.
Our suggestions show that it is possible to easily perform statistical inference of $\ddcq$, whilst properly accounting for the AE uncertainty and better controlling the false positive rate.
\bigskip \\
\textbf{Keywords:} amplification efficiency; delta-delta Cq; $\ddcq$; efficiency adjusted; power calculation; qPCR
\end{abstract}


\section{Introduction}
Despite being an aging technique, real-time quantitative polymerase chain reaction (qPCR)---arguably one of the most significant biotech discoveries of all time---is still heavily used in molecular biology \citep{Rainbow1996}.
qPCR is an extremely sensitive and cost-effective technique to amplify and quantitate the abundance of DNA and even mRNA by using so-called reverse transcriptase.
In life sciences, qPCR is typically applied to quantify candidate gene transcripts that are biomarkers of diagnostic, prognostic, and even predictive value in e.g.\ infectious diseases and cancer.
In the slip stream of high-volume omics-data, another very important application of qPCR has arisen.
Here qPCR is the gold standard validation tool for the most promising gene transcripts generated by high-throughput screening studies such as microarrays or sequencing.
For validation experiments in particular the ability to control the type I error rate is very important.
Unfortunately, important statistical details are often omitted resulting in a failure to obtain the desired type I error probability.
Validation without such an ability cannot be considered very meaningful and therefore conservative approaches should be taken.

Like all experiments in molecular biology and chemistry, qPCR is sensitive not only to genes and gene transcripts of interest, but also laboratory settings and experimental conditions.
There is an incredible amount of sources of systematic and non-systematic variation including temperature variations, concentration differences by pipetting errors, and primer affinity, in addition to the genuine biological variations of interest across case and control samples.
Laboratory guidelines and increasingly sophisticated statistical modelling have been established to combat many of these systematic errors \citep{Bustin2010, Bustin2009, Spiess2015}.

The so-called $\ddcq$ quantity is the normalized relative expression of a target gene of interest between treated (case) and untreated samples (control) accounting for undesired variations using one or more endogenous reference genes (also called housekeeping gene) assumed to be approximately unchanged due to the treatment.
The $\ddcq$-value is usually based on the assumption of perfect AEs for both the target and reference gene.
However, the target and reference genes might be subject to different AE which yield biased $\ddcq$-values.
In turn, the $\ddcq$ has been modified to AE corrected versions \citep{Pfaffl2001, Rao2013, Rao2013b}.

Despite the tremendous success of qPCR, `statistical inference considerations are still not accorded high enough priority' \citep{Bustin2009, Bustin2010}.
We find this particular true for the estimation of the AE.
Although efficiency calibration has been extensively treated by \cite{Pfaffl2001} or in the more generalized model by \cite{Yuan2008}, there seems to be a lack of systematic studies of the unavoidable influence of the uncertainty of the AE estimate on the conclusions of qPCR experiments based on formal statistical inference.
The current AE adjusted $\ddcq$ methods do not account for the \emph{uncertainty of the estimated AE} and thus effectively assumes the AE to be estimated with infinite precision.
This assumption implies a systematic underestimation of the standard error of $\ddcq$ leading to too narrow confidence intervals, decreased $p$-values, and thereby increased type I error rates.
If the AE is poorly determined this underestimation can drastically increase the standard error of $\ddcq$ and similar quantities.

\citet{Nordgaard2006} studied error propagation including the effect of the AE uncertainty on the $C_q$-values.
This study was, however, statistically informal and made no attempt to quantify the effect on the $\ddcq$ and inference hereof.
Furthermore, \citet{Nordgaard2006} considered AE estimation from the amplification curve (thus for each sample) and not from separate dilution experiments.
In this paper, we discuss only the AE from dilution curves explicitly.
However, similar problems exist as the AE estimates from the amplification curves \emph{also} have an associated standard error which affect the `down-steam' quantities and inference.

\citet{Svec2015} recently assessed the impact of the AE uncertainty as a function of the number of technical replications at each concentration and the qPCR instrument.
They conclude that a minimum of $3$--$4$ replicates at each concentration are needed and that a significant inter qPCR instrument effect is present.
However, they do not gauge the effect of the number of concentrations used---an important variable as additional technical replicates rarely contribute with much information to determine the AE.
Nonetheless, \citet{Svec2015} do not address the impact of the AE uncertainty on formal statistical inference on the $\ddcq$, as this paper intends.

% Precise aims of the paper
\subsection{Aims}
Primarily, we aim to highlight the common problem of disregarding the uncertainty of the AE estimate in statistical inference of the $\ddcq$-value in qPCR experiments.
And we propose and benchmark different off-the-shelf and novel solutions to this problem.

To this end, we employ a statistical model which allows such formal inference.
This covers statistical model formulation, confidence intervals, hypothesis testing, and power calculation, with special emphasis on false positive rates.
Simultaneous estimation of the uncertainty of the AE estimate and mean $C_q$-values by linear mixed effects models (LMM), which allows a more appropriate handling of the technical and sample errors, is described.
We investigate the use of the statistical delta method, Monte Carlo integration, or bootstrapping to correctly perform inference on the value of $\ddcq$.

Note two important observations:
First, the problem exists for \emph{all} statistical models and methods which incorrectly disregard the uncertainty of the AE estimate and is not limited to LMMs.
Second, the problem exists not only for $\ddcq$-values, but also all similar quantities, e.g.\ $\Delta C_q$ and $C_q$, and the statistical inferences based on these.

The idea of using LMMs for qPCR experiments is not new \citep{andersen2004,abruzzo2005,fu2006,Steibel2009,Matz2013}.
\cite{andersen2004} and \cite{abruzzo2005} have used mixed effects modeling to identify candidate normalizing genes.
\citet{fu2006} applied the related generalized estimation equations to handle intra and inter group variation.
However, the usage of LMMs combined with the statistical delta method, Monte Carlo integration, or bootstrapping to handle uncertainty stemming from the efficiency estimation seems to be novel and provides a general statistical framework for qPCR experiments and may be considered an extension of the strategy by \citet{Yuan2008}.
\citet{Steibel2009} and \citet{Matz2013} use the mixed models primarily for the $C_q$-value estimation.

We demonstrate that considering the uncertainty of the AE is, unsurprisingly, highly important when the AE is determined with inadequate precision and vice versa.
We do so by three application examples and a simulation experiment.
In the first two applications, the consideration of the AE uncertainty is largely unimportant for $\ddcq$ inference due to a large number of dilution steps and well-determined AE.
In the last application, we see that the AE uncertainties have a drastically different impact on $\ddcq$ inference.
In a simulation study, we show that the methods proposed indeed control the false positive rate better than the conventional approach and provide further insight into the problem.

In the first application, we also verify that multiple myeloma cancer cell lines differentially express the \textit{MGST1} gene depending on the abundance of culture initiating cells.
In the second application, the approaches are also used to design and analyze a study which results turned out to support the hypothesis of \cite{Robertus2009} that miRNA-127 is differentially expressed between testicular and nodal DLBCL.


\section{Methods}

\subsection{Observational model}

In order to approximate the standard error of the AE adjusted
$\ddcq$ we model the amplification process
in the following way
\begin{equation}
  \label{eq:fluorescencemodel}
  F_{C_q} = \kappa \sigma N_0 (2^{\alpha})^{C_q},
\end{equation}
where $F_{C_q}$ is the fluorescence measurement at the $C_q$'th cycle,
$\kappa$ is a proportionality constant, $N_0$ is the number of transcripts of interest in the initial sample before amplification,
$\sigma$ is the sample specific handling error and
$\alpha$ is the percentage of the $\log_2$-AE.
In practice, the transcript abundance level is determined by the cycle $C_q$ for which a given fluorescence measurement $F_{C_q}$ is reached.
We rearrange \eqref{eq:fluorescencemodel} and notice that $C_q$ can be expressed as
$
  \alpha C_q = \log_2 F_{C_q} - \log_2 \kappa \sigma N_0.
$
In order to estimate the relative amount of target (tgt) gene transcripts between case
and control (ctrl) samples, we assume
the amount of the reference (ref) gene template is the same in both the case
and the control,
$N_{0,\rref,\case}=N_{0,\rref,\ctrl}$,
and that the AE only vary between the target and reference gene. We then
arrive at the following expression for the $\log_2$-fold change of
the target gene template between case and controls:
\begin{align*}
  \log_2\Bigl(\frac{N_{ 0,\tgt,\case}}{N_{0,\tgt,\ctrl}}\Bigr)
  &= \log_2 \kappa \sigma_{ \case} N_{0,\tgt,\case}
    -\log_2 \kappa \sigma_{ \case} N_{0,\rref,\case} \\
  &\quad
       - \log_2 \kappa \sigma_{\ctrl} N_{0,\tgt,\ctrl}
       + \log_2 \kappa \sigma_{\ctrl} N_{0,\rref,\ctrl}  \\
  &=   - \bigl\{(\alpha_{\tgt}  C_{ q,\tgt,\case}
       -  \alpha_{\rref} C_{q,\rref,\case})
       - (\alpha_{\tgt}  C_{q,\tgt,\ctrl}
       - \alpha_{\rref} C_{q,\rref,\ctrl})\bigr\},
\end{align*}
assuming that the $C_q$-values have been determined by a common florescence level $F_{C_q}$.
This method of estimating the log relative abundance between case and controls is often referred to as the $\ddcq$-method \citep{Livak2001}, after the double difference appearing in the expression:
\begin{align}
  \label{eq:ddcq}
  \ddcq :=
    (\alpha_{ \tgt}C_{ q,\tgt,\case} - \alpha_{ \rref}C_{ q,\rref,\case})
      - (\alpha_{ \tgt}C_{ q,\tgt,\ctrl} - \alpha_{ \rref}C_{ q,\rref,\ctrl}).
\end{align}
Thus we have $2^{-\ddcq}$ as the relative abundance of the original target transcript corrected for the AE.



\subsection{Statistical model}
We study the problematic aspects of ignoring the uncertainty of the AE estimate.
Note, however, that this problem persists for \emph{all} statistical models and methods which na\"{i}vely `plug-in' the AE estimate from dilution curves into formulae concerning the $\ddcq$.

For ease of notation we use the abbreviations
$i\in\{\tgt,\rref\}$ for gene types target and reference;
$j\in\{\case,\ctrl,\std\}$ for sample types case, control, and standard curve;
$s\in\{1,\dots,n_{ij}\}$ for samples in the $ij$'th group; and
$k\in\{0,\dots,K_{ijs}\}$ for dilution steps for each sample.
To estimate $\ddcq$ of \eqref{eq:ddcq}, estimates of $\alpha_i$ are needed.
A popular way of estimating the AE is by ordinary linear regression.
I.e.\ by regressing $C_{q,ij}$ against a series of increasing values $0 = x_1 < \cdots < x_K$, defined by $N_{0,ijk} = N_{0,ij}2^{-x_k}$, and na\"{i}vely plugging $\hat{\alpha}_i$ into \eqref{eq:ddcq} and thus disregarding its uncertainty.
Here, $k$ denotes the dilution step and $x_k$ the number of 2-fold dilutions (e.g.\ $x_1 = 1$ means the first dilution step halves the original concentration).
The estimation of the expected $C_{q,ij}$-values and $\alpha_i$ can then be estimated simultaneously when formulated as a LMM \citep{Pinheiro2000a};
\begin{equation}
  \label{eq:model}
  C_{q,ijsk} = \mu_{ij} + A_{js} + \gamma_{i} x_k + \epsilon_{ijsk},
\end{equation}
where $A_{js}$ is a random effect from sample $s$ under the $j$'th sample type,
$\gamma_{i} = \alpha_i^{-1}$, and
$\mu_{ij}$ is the group means.
The random effects $A_{js}$ are ${\cal{N}}(0,\sigma_S^2)$-distributed and
the error terms $\epsilon_{ijsk}$ are independent and ${\cal{N}}(0,\sigma^2_j)$-distributed with a sample type specific variance $\sigma^2_j$.
The random effects account for the paired samples across tgt/ref for each $j$.
LMMs provide a more correct quantification of the sources of variation and thereby a more correct estimate of the uncertainty of $\mu_{ij}$ and their derived quantities.

In one application we shall relax the assumption that the AE is independent of $j$ and consider group-specific AEs $\alpha_{ij} = \gamma_{ij}^{-1}$.

Although, variation due to technical replicates should be modeled in \eqref{eq:model} as an additional random effect term, we average out technical replicates for simplicity.
For further simplicity of this paper, we refrained from using multiple reference genes simultaneously in the $\ddcq$ estimation although our the framework and methods easily extends to this case.


\subsection[Inference for DDCq by the delta method and Monte Carlo integration]{Inference for $\ddcq$ by the delta method and Monte Carlo integration}

We first consider hypothesis testing and confidence intervals for $\ddcq$ by the statistical delta method.
Let the maximum likelihood estimates of the fixed effects
\begin{equation*}
  \vtheta = (\mu_{\tgt,\case},  \mu_{\tgt,\ctrl},  \gamma_\tgt,
             \mu_{\rref,\case}, \mu_{\rref,\ctrl}, \gamma_\rref)^\top
\end{equation*}
be denoted by
$
  \hat{\vtheta} =
    (\hat{\mu}_{\tgt,\case},  \hat{\mu}_{\tgt,\ctrl},  \hat{\gamma}_\tgt,
     \hat{\mu}_{\rref,\case}, \hat{\mu}_{\rref,\ctrl}, \hat{\gamma}_\rref)^\top.
$
We wish to test the hypothesis
$
  H_0 : c(\vtheta) = 0,
$
where $c$ is the continuously differentiable function of the
fixed effects given by
\begin{equation}
  \label{eq:c}
  c(\vtheta)
    =      \big\{ (\mu_{\tgt,\case} \gamma_\tgt^{-1}
               -  \mu_{\rref,\case} \gamma_\rref^{-1})
               - (\mu_{\tgt,\ctrl}  \gamma_\tgt^{-1}
               -  \mu_{\rref,\ctrl} \gamma_\rref^{-1})\bigr\}.
\end{equation}
The main task of this paper is to approximate the standard error of $c(\hat{\vtheta})$ and thereby account for the uncertainty of $\ddcq$.
That is, the standard error,
\begin{equation}
  \label{eq:se}
  \mathrm{se}(\hat{\vtheta}) = \sqrt{\Var\!\bigl[c(\hat{\vtheta})\bigr]},
\end{equation}
is of central interest.
The standard error is used in the statistic for testing $H_0$ given by
$t = c(\hat{\vtheta})/\mathrm{se}(\hat{\vtheta})$,
which according to a first order Taylor series expansion of $c$ can be
approximated by
\begin{equation}
  t =
  \frac{
    c(\hat{\vtheta})
  }{
   \sqrt{\nabla c(\hat{\vtheta})^\top
        \Var[\hat{\vtheta}]
        \nabla c(\hat{\vtheta})}
  }.
  \label{eq:tstat}
\end{equation}
According to \citet[Section 2.4.2]{Pinheiro2000a}, $t$ is approximately
$t$-distributed with $\eta$ degrees of freedom.
The degrees of freedom of multilevel mixed effects models are non-trivial to obtain in general.
We do not pursue this further and restrict ourselves to the case of balanced experimental designs where $\eta$ is obtained relatively straight-forwardly.

On the basis of \eqref{eq:tstat}, an approximate $(1 - \alpha) 100\%$ confidence interval of $c(\vtheta)$ can then be given by
\begin{equation*}
     c(\hat{\vtheta}) \pm
     t_{\alpha/2,\eta}\sqrt{\nabla c(\hat{\vtheta})^{\top}\Var[\hat{\vtheta}]\nabla c(\hat{\vtheta})}.
\end{equation*}
Likewise, $p$-values can be obtained by computing $P(\lvert t \rvert > T)$ where $T$ is $t$-distributed with $\eta$ degrees of freedom.

Alternatively to \eqref{eq:tstat}, the variance $\Var\!\bigl[c(\hat{\vtheta})\bigr]$ can be evaluated by Monte Carlo integration.
One way is to simulate a large number $N$ of parameters $\vtheta_1, \ldots, \vtheta_N$ from a multivariate normal distribution using the estimated parameters ${\cal{N}}_6(\hat{\vtheta}, \Var[\hat{\vtheta}])$ and compute the sample variance of $c(\vtheta_1), \ldots, c(\vtheta_N)$.

Both maximum likelihood (ML) and restricted maximum likelihood estimation (REML) of LMMs is implemented in the R-packages \texttt{lme4} and \texttt{nlme} \citep{Bates2014, Pinheiro2000a}.
The packages readily provides the estimate $\hat{\vtheta}$ and $\Var[\hat{\vtheta}]$ and we use these in the construction of test and confidence intervals for the $\ddcq$ as described above.
The needed gradient in \eqref{eq:tstat} is computed straight-forwardly from \eqref{eq:c}.
% \begin{equation*}
%   \nabla c(\hat{\vtheta}) =
%   \left(
%     \gamma_\tgt^{-1} ,
%     -\gamma_\tgt^{-1} ,
%     \mu_{\tgt,\ctrl} \gamma_\rref^{-1} - \mu_{\tgt,\case} \gamma_\tgt^{-1} ,
%     -\gamma_\rref^{-1} ,
%     \gamma_\rref^{-1} ,
%     \mu_{\rref,\case} \gamma_\tgt^{-1} - \mu_{\rref,\ctrl} \gamma_\rref^{-1}
%  \right)^\top.
% \end{equation*}

We note that the division by $\gamma_j$ in \eqref{eq:c} is problematic as $\hat{\gamma}_j$ is normally distributed and values near zero can increase the variance dramatic.
In practice, this is only problematic if the standard error of $\hat{\gamma}_j$ is sufficiently large.
One way to solve this problem is to use the $\log_2$ concentration as the response and the $C_q$-values as the explanatory variables in a regression model of the standard curve to estimate $\alpha_j$ directly.
This approach is not without conceptual problems as this puts the errors on the explanatory variables.
To this end, note that the hypothesis
$
  H_0: \gamma_\tgt\gamma_\rref c(\hat{\vtheta}) = 0,
$
can be equivalently tested
for which the standard error of the test-statistic can be worked out exactly.
% We omitted doing so for two reasons;
% First, the $\ddcq$-values is the well-known and understood quantity for non-statistician researchers.
% Secondly, the calculations involve higher-order tensors not easily understood.

If $\gamma_\tgt^{-1}$ and $\gamma_\rref^{-1}$ are assumed to be one (or otherwise known) then \eqref{eq:c} becomes a simple linear hypothesis for which the standard error is easily calculated.
This corresponds to leaving out the terms in \eqref{eq:model} involving these parameters and thus ignoring dilution data.
If $\gamma_\tgt^{-1} = \gamma_\rref^{-1} = 1$ is assumed, we shall refer to the obtained estimate as the na\"{i}ve LMM.
If $\gamma_\tgt^{-1}$ and $\gamma_\rref^{-1}$ are assumed known (i.e.~disregarding the standard error hereof) we refer to the obtained estimate as the efficiency corrected (EC) estimate.
The estimate where the uncertainty of the AE is considered is referred to as efficiency corrected and variance adjusted by either the delta method (EC\&VA1) or Monte Carlo integration (EC\&VA2).


\subsection[Inference for DDCq by the bootstrap method]{Inference for $\ddcq$ by the bootstrap method}
\label{sec:bootstrap}

We now consider hypothesis testing and confidence intervals for $\ddcq$ by bootstrapping as an alternative approach.
The bootstrap, which avoids calculating gradients, is often cited to perform better in small sample situations \citep{Efron1982}.

The basic idea of the bootstrap is that inference on $\ddcq$ can be conducted by re-sampling the sample data and performing inference on the re-sampled data.
In the usual qPCR setup with paired samples and dilution data, straight-forward bootstrapping will quickly fail.
We propose non-parametric block bootstrap samples for the case-control data generated by sampling matched pairs of tgt/ref genes with replacement for cases and controls, respectively.
However, as we have only got a single observation for each dilution step we chose to re-sample residuals from a simple linear regression model and subsequently adding the residuals to the fitted values from the linear regression.
Hence the $B$ bootstrapped datasets consists of the re-sampled matched pairs and the residual bootstrapped standard curve.
For each dataset, $\hat{\delta}_1 = \ddcq^{(1)}, \ldots, \hat{\delta}_B = \ddcq^{(B)}$ are computed to obtain the bootstrap distribution from which confidence intervals and $p$-values can be obtained.
The standard error of $\ddcq$ is estimated by the sample standard deviation of the bootstrap distribution.
A $(1-\alpha)100\%$ confidence interval can be computed as
$ % \begin{equation*}
  ( \hat{\delta}_{(\alpha/2)} , \hat{\delta}_{(1-\alpha/2)} )
$ % \end{equation*}
where e.g.\ $\hat{\delta}_{(\alpha/2)}$ denotes the $\alpha/2$-percentile of $\hat{\delta}_1, \ldots, \hat{\delta}_B$.
The $p$-value for the null hypothesis of $\delta = 0$ is computed by
\begin{equation*}
  2\min(\pi, 1 - \pi)
  \text{ where }
  \pi = \frac{1 + \sum_{i = 1}^B \bbOne[\hat{\delta}_i \leq 0]}{B + 1}.
\end{equation*}

While the bootstrap is an intuitive and excellent method for estimating the standard error, it quickly becomes computationally heavy.
The rather complicated designs of qPCR experiments with paired samples, dilution data, and other random effects also makes the bootstrap less attractive as good bootstrap sampling schemes are hard to produce.

Alternatively, parametric bootstrap can be used by simulating datasets from the fitted model.
Here, both new random effects and noise terms are realized and added to the fitted values to generate new datasets.

% In the following real data data applications, we draw \Sexpr{n.boots} bootstrap samples (both parametric and non-parametric) and used them to carry out statistical inference.
% First, the standard error of $\ddcq$ is calculated by the sample standard deviation of the bootstrap distribution.
% Secondly, the $p$-value for the test for the null hypothesis of a vanishing $\ddcq$ is calculated as the minimum of two times the percentile and two times one minus the percentile of 0 in the bootstrapped sample.



\section{Applications}

We applied the described approaches to two qPCR validation experiments regarding culture initiating cells (CICs) in multiple myeloma (MM) and non-coding microRNAs in diffuse large B-cell lymphoma (DLBCL).
In both experiments, the $C_q$-values were extracted for both the reference and target transcripts with automatic baseline and threshold selection \citep{Mx300P2013}.
We also illustrate the method on a public available qPCR dataset concerning the differential gene expression in arabidopsis thaliana grown under different conditions.
In order to gauge the performance of the methods we subsequently performed a simulation study.

\subsection{CIC study}

\subsubsection{Introduction}
A cell is culture initiating if it can initiate a sustained production of cells when cultured in vitro.
The viability potential of a cell population can be assessed by measuring the number of culture initiating cells (CICs).
This number can be estimated by a dilution experiment where cells are seeded in decreasing numbers.
The ratio of CICs can then be estimated by e.g.~Poisson regression \citep{Lefkovits1999}.
CICs are of particular interest in cancer research as cancers with high culture initiating potential seemingly have stem cell like properties making them resistant towards chemotherapy \citep{Chen2013}.

In search for genes associated with a high culture initiating potential in MM we made limiting dilution experiments of 14 MM cell lines and divided them into 7 cell lines with low and 7 cell lines with high culture initiating potential.
Gene expression profiling by microarrays identified genes \textit{MGST1} and \textit{MMSET} to be differentially expressed between cell lines with high and low abundance of CICs.
As gene expression detection by microarrays can be hampered by high false positive rates, the purpose of this experiment was to validate the findings of the association of \textit{MGST1} and \textit{MMSET} with culture initiating potential by qPCR.


\subsubsection{Sample and data preparation}

For this, 8 MM cell lines (AMO-1, KMM-1, KMS-11, KMS-12-PE, KMS-12-BM, MOLP-8, L-363, RPMI-8226) with $>10\%$ CICs, and 8 MM cell lines (ANBL-1, KAS-6-1, LP-1, MOLP-2, NCI-H929, OPM-2, SK-MM-2, U-266) with $<1\%$ CICs were used.
The fraction of CICs was determined by the limiting dilution method, see \cite{Lefkovits1999}. Total RNA was isolated from frozen cell culture pellets, using a combined method of Trizol (Invitrogen) and Mirvana spin columns (Ambion).
Isolated RNA was reversed transcribed into complementary DNA (cDNA) synthesis using SuperScript III First-Strand Synthesis Supermix (Invitrogen).
As input into the total cDNA synthesis of 250ng total RNA was used. Equal amounts of random hexamers and oligo(dT) were used as primers.
Quantitative real-time reverse transcriptase polymerase chain reaction was performed on a Mx3000p qPCR system (Agilent Technologies/Stratgene) using the TaqMan UniversalPCR Master Mix, No AmpErase UNG, and TaqMan gene expression Assays (Applied Biosystems).
The following TaqMan Gene Expression Assays were used (Assay ID numbers in parentheses): \textit{MGST1} (Hs00220393\_m1), \textit{MMSET} (Hs00983716\_m1).
The two reference genes beta-actin (\textit{ACTB}) and \textit{GAPDH} were used as endogenous controls, assay IDs 4333762-0912030 and 4333764-1111036, respectively.
For each target and reference transcripts a standard curve based on seven 2-fold dilutions was constructed on a reference sample consisting of material from the AMO-1 cell line.

\subsection{DLBCL study}

\subsubsection{Introduction}
The association between oncogenesis and micro RNAs (miRNAs), short non-coding RNA transcripts with regulatory capabilities, has recently prompted an immense research activity. The possibility to change treatment strategies by transfecting antisense oligonucleotide to control abnormally up-regulated miRNAs in malignant tissue is of particular interest \citep{Garzon2010}. In that respect up-regulated \textit{miR-127} and \textit{miR-143} in testicular DLBCL have shown treatment changing potential \citep{Robertus2009}. However, as the number of screened miRNAs was high and the sample size was low in Robertus et al.'s work invoking high risk of false discoveries we set out to validate the differential expression of \textit{miR-127} and \textit{miR-143} in tissues from our own laboratory using our improved qPCR analysis workflow.

\subsubsection{Sample and data preparation}

For this study, DLBCL samples were collected from 8 testicular (case) and 8 nodal (control) paraffin embedded lymphomas at Aalborg University Hospital.
The samples were collected in accordance with a research protocol accepted by the Health Research Ethics Committee for North Denmark Region (No.~N-20100059).
Total RNA was isolated using a combined method of Trizol (Invitrogen) and Mirvana spin columns (Ambion). An amount of 10ng total RNA was synthesized into first strand cDNA in a 15$\mu$L reaction using TaqMan MicroRNA Reverse Transcription Kit (Applied Biosystems) according to the manufactures instruction. In total 1.33$\mu$L cDNA was used as template in the real time PCR amplification performed by Mx3000p QPCR system (Agilent Technologies/Stratgene) with sequence specific TaqMan primers (Applied Biosystems).
As reference transcripts we chose \textit{RNU-6B} and \textit{RNU-24}, which were less variable and equally expressed across nodal and extra-nodal samples among a larger list of candidate reference genes.
For each target and reference transcripts a standard curve based on seven 2-fold dilutions was constructed on a reference sample consisting of pooled material from all 16 lymphomas.


\subsection{Arabidopsis thaliana study}

\subsubsection{Introduction}
In order to illustrate the effect of applying variance approximations in a dataset with a limited number of dilution steps and samples we considered the arabidopsis thaliana dataset published by \citet{Yuan2008}.
The dataset contains one gene of interest, \textit{MT7}, potentially differentially expressed under two growth conditions of the plant arabidopsis thaliana and two reference genes ubiquitin (\textit{UBQ}) and tublin.


\subsubsection{Sample and data preparation}
The arabidopsis thaliana plant growth, RNA extraction, and qPCR experiments were carried out as described in \citet{Yang2006}. The cDNA was diluted into 1-to-4 and 1-to-16 serial dilutions. Real-time PCR experiments was performed in duplicates for each concentration \citep{Yuan2008}.

Due to the the study design, we naturally fitted estimation efficiencies $\gamma_{ij} = \alpha_{ij}^{-1}$ for each group.
Because of the few samples we omitted the, in this case, meaningless random sample effect of the LMM.


\subsection{Simulation study}

In order to properly benchmark statistical test procedures one needs to have an idea of the false positive rate (FPR), or type I error rate, as well as the true positive rate (TPR), or sensitivity.
As ground truth is usually not available in non-synthetic data, we use simulation experiments to determine the error rates of the discussed statistical procedures.

In our setting, the FPR of a statistical test is the probability that the test incorrectly will declare a result statistically significant given a vanishing effect size or difference of $ c(\theta) = 0$ between case and controls; {i.e.}
$ % \begin{equation*}
  \mathrm{FPR} =
    P\bigl(\vert t \vert > t_{1 - \alpha/2,\eta} \bigm| c(\theta) = 0\bigr).
$ % \end{equation*}
On the other hand the TPR of the statistical test is the probability that the test will correctly declare a result statistically significant given an non-zero effect size $\delta = c(\theta)$ between case and controls; {i.e.}
$ % \begin{equation*}
  \mathrm{TPR} =
    P\bigl(\vert t \vert > t_{1 - \alpha/2,\eta} \bigm| c(\theta) = \delta\bigr).
$ %\end{equation*}

A straightforward way to obtain an estimate of the TPR is to simulate a large number $n$ of datasets under the alternative hypothesis of $c(\theta) = \delta$, fit the model for each dataset, and compute $t$-values $t_1, \dots, t_n$.
From these $t$-scores the TPR can estimated by
\begin{equation*}
  \widehat{\mathrm{TPR}} =
    \frac{1}{n}\sum_{i=1}^n
      \bbOne\bigl[\vert t_i \vert > t_{1-\alpha/2,\eta} \bigr],
\end{equation*}
where $\bbOne[\;\cdot\;]$ is the indicator function.
Hence, the estimated TPR is the fraction of tests correctly declared significant.

Likewise, an estimate of the FPR is obtained by simulating $n$ datasets under the null hypothesis of $c(\theta) = 0$ and obtaining $t$-values $t_1, \dots ,t_n$ from which FPR is estimated by
\begin{equation*}
  \widehat{\mathrm{FPR}} =
    \frac{1}{n}\sum_{i=1}^n
      \bbOne\bigl[ \vert t_i \vert >  t_{1 - \alpha/2,\eta} \bigr],
\end{equation*}
i.e.\ the fraction of tests incorrectly declared significant.

Based on the above statistical model, we estimate the FPR and the TPR for each discussed method under different choices of sample sizes and number of dilutions whilst fixing the size of the sample and experimental variations.


\section{Results}

\subsection{CIC study}
The $C_q$-values and dilution curves for the CIC study are depicted in Fig.~\ref{fig:cqCIC} panels A--B, respectively.
The simple linear regressions show well-determined standard curves with small standard errors on the estimate of the slopes.

The values of the considered estimators for $\ddcq$ are seen in Table~\ref{table:cic}.
The table also shows results of tests for difference in gene expression assessed by the $\ddcq$ for both target genes \textit{MGST1} and \textit{MMSET} normalized to each of the reference genes \textit{GAPDH} and \textit{ACTB}.
We used four different methods to estimate and perform inference:
  (1) EC: Efficiency corrected LMM estimate ignoring the uncertainty of the efficiency estimates.
  (2) EC\&VA1: EC and variance adjusted LMM estimate using 1.~order approximation.
  (3) EC\&VA2: EC and variance adjusted LMM estimate using Monte Carlo integration.
  (4) Bootstrap: Estimate by the bootstrap described in Section \ref{sec:bootstrap} fitting the LMM and using the EC estimate.

Consider the first section of Table~\ref{table:cic} where tgt \textit{MGST1} is normalized against the reference \textit{GAPDH}.
The tests for a vanishing $\ddcq$ are all highly significant with comparable $95\%$ CIs.
As expected, the efficiency corrected estimates are unchanged due to the variance adjustment,
and only the standard deviation of the estimate is increased.
The increase of the standard error is very small resulting in small but unimportant increases of the absolute $t$- and $p$-values.
The results remain significant for the \textit{MGST1} gene.
Very similar results are obtained if \textit{ACTB} is used as reference.
In conclusion, there is good evidence that \textit{MGST1} is differentially expressed between cell lines with high and low abundance of CICs.

For the target gene \textit{MMSET} normalized with respect to both reference genes, all estimates are not significantly different from zero.
Again, the various methods all agree and no substantial inter-method differences are seen and we find no evidence for differential expression of \textit{MMSET} between cell lines with high and low abundance of CICs.

In all instances, our bootstrap scheme provides a standard deviation larger than what is obtained using the delta or Monte Carlo methods.
The mean of the bootstrap distribution seems consistently larger that the other methods.

We see that the large number of dilution steps, as recommended and expected, ensures a low impact of the AE on the standard error and thus on the inference of the $\ddcq$.




\subsection{DLBCL study}

The $C_q$-values and dilution curves for the DLBCL study are depicted in Fig.~\ref{fig:cqTestis}, panels A--B, respectively.
Analogous to the previous section, the differences in gene expressions assessed by the $\ddcq$ for the target genes \textit{miR-127} and \textit{miR-143} with respect to each reference gene rnu6b and rnu24 were estimated using the four different methods.
Again $\Sexpr{n.boots}$ bootstrapped samples were used.
The results are seen in Table~\ref{table:tesits}.

We notice the efficiency corrected estimates are exactly equal with and without variance adjustment, while the standard deviation of the estimate and the $p$-values are higher for the adjusted values as expected.
The size of the increase is again undramatic hinting at well determined AE using the dilution curves.

For all combinations of reference genes the estimates for \textit{miR-127} are significantly different from zero at the usual 5 \% significant level, but not at the 1 \% significance level.
The \textit{miR-143} estimates are not significantly different from zero.
Despite the very small increase in standard error, the $p$-values increase at the second digit.

The bootstrap method provides a standard deviation similar to the delta method and Monte Carlo integration for both \textit{miR-127} and \textit{miR-143}.

Regarding the biological interest, we conclude there is evidence for a difference in \textit{miR-127} expression between testicular and nodal DLBCL whilst the data is not compatible with difference in \textit{miR-143} expression.
While the AE estimate had no influence in these cases a change in significance is easily imagined in other cases.



\subsection{Arabidopsis thaliana data}

The $C_q$-values and dilution data for the arabidopsis thaliana data are shown in Fig.~\ref{fig:cqYuan}.

The estimated difference in gene expression between case and control of the target gene \textit{MT7} normalized to either reference (Tublin or \textit{UBQ}) is seen in Table~\ref{table:yuan}.
The table shows the efficiency corrected method with and without variance adjustment by the delta method.
In both cases, we see a dramatic increase in the standard error, $p$-values, and size of the confidence intervals.
When using variance adjustment there is no longer a highly statistical significant difference in \textit{MT7} expression between case and ctrl growth conditions.

The results may be surprising at first sight when considering the relatively small standard errors of the slopes in the simple linear regressions shown in Fig.~\ref{fig:cqYuan}.
One might imagine that the uncertainty of the AE is negligible and thus perform the usual analysis.
However, we see the contrary for several reasons.
First, using only 3 dilutions steps leaves very few degrees of freedom left in each group as we are left with few samples and a high number of parameters to be estimated.
Secondly, as dilution curves are used for each group the four group-specific AE estimates will all contribute to increasing the standard error of the $\ddcq$.
While this example was selected as a worst-case scenario, it should illustrate that although the standard curves are seemingly well determined, it is hard to intuitively predetermine the combined effect on the standard error of $\ddcq$.

We note here, that no pre-averaging of the technical replicates for each concentration was done.
Instead, the technical replicates where modeled as a random effect.




\subsection{Simulation study}

First, we present results of a simulation study for a two-sided test for the null hypothesis of a vanishing $\ddcq$ at a $5\%$ significance level.
We simulated \Sexpr{n.sims} datasets under both the null and alternative hypothesis with 6 samples in each case and control group and standard curves with 6 dilution steps.
The effect size under the alternative was set to $\delta = 10/9$.
The sample and experimental standard deviations were set to $\sigma_{S} = 1$ and $\sigma = 1$, respectively.
The AE for the target and reference genes were set to 0.80 and 0.95, respectively.

The four discussed methods were applied to the $2\times \Sexpr{n.sims}$ datasets and the $p$-value testing the null hypothesis were computed.
The results of these tests are summarized in Table \ref{tab:simexample} from which the FPR and TPR can be computed at the $5$\% cutoff.
From Table \ref{tab:simexample}, we see the estimated FPRs are \Sexpr{ex.fpr[1:2]}, and \Sexpr{ex.fpr[3]} for the efficiency corrected LMM (EC), the efficiency corrected LMM with variance adjustment using the delta method (EC\&VA1), and the bootstrap, respectively.
We omitted EC\&VA by Monte Carlo integration here due to the computational cost and the similar results with EC\&VA1 in the previous.
As expected, the EC method does not control the FPR at the $5\%$-level.
The variance adjusted estimator is consistent with controlling the FPR at the $5\%$ level.
By construction, the variance adjusted will always perform at least as good as the EC in terms of FPR.
Surprisingly, the bootstrap has the worst performance in terms of FPR.

Secondly, the TPR are estimated to be \Sexpr{ex.tpr[1:2]}, \Sexpr{ex.tpr[3]} for three methods \Sexpr{names(ex.tpr)}, respectively.
As expected, we notice that an improved FPR comes a the cost of a decreased TPR for a given statistical procedure.

The above simulations were repeated for sample sizes 4 or 8 in both case and control groups in combination with 4 or 8 dilution steps with the same simulation parameters.
Fig.~\ref{fig:simstudy} shows the performance of the methods in terms of FPR and TPR.
Each panel corresponds to a given number of samples and dilutions.
In each panel the $p$-value cut-off is varied between 0.01, 0.05, and 0.1.
Overall, we see that the EC\&VA estimate is the only procedure consistent with controlling the FPR at the nominal chosen significance level.
Likewise, for many dilutions, the difference between the EC and EC\&VA procedures diminish as the uncertainty of the AE is relatively low.
As expected a decrease in FPR corresponds to a decrease in TPR.

To gauge when the standard error of \eqref{eq:se} is determined with adequate precision, we simulated $2\times \Sexpr{n.sims}$ datasets and computed the mean standard error of the $\ddcq$ for the EC and EC\&VA procedures as a function of the number of dilutions and samples.
We varied the number of dilutions in the range \Sexpr{paste0(range(dils), collapse="--")} for a number of samples in the range \Sexpr{paste0(range(samps), collapse = "--")} with the same settings as above.
Fig.~\ref{fig:simstudy2} shows these results.
As expected, increasing the number of samples or the number of dilutions yield a smaller standard error.
Also unsurprising and as already seen in the applications, the differences in the standard error for the EC and EC\&VA methods are very substantial for a small number of dilutions and vanish as the number of dilutions steps increase.
The differences in the standard error seems to be larger under the alternative than the null hypothesis.
Similar figures might also aid in designing qPCR experiments and help determine if investing in additional dilutions or samples is preferable---obviously with properly chosen simulation parameters in the given context.

\section{Discussion and conclusion}
% Main message
The commonly used efficiency corrected $\ddcq$ approach to analysis of qPCR data disregards the uncertainty of the estimated AE leading to increased false positive rates.
As qPCR experiments are often used for validation this is highly undesirable.
Our primary approach based on the statistical delta-method to approximate the variance of the efficiency adjusted $\ddcq$, shows that it is possible to perform statistical inference about qPCR experiments whilst more properly accounting for the AE uncertainty.
We also note that the problem is not limited to the $\ddcq$ statistic.

The approach was used to validate that:
(1) \textit{MGST1} is differentially expressed between MM cell lines of high and low abundance of CICs and
(2) analyze and study the hypothesis that miRNA-127 is differentially expressed between testicular and nodal DLBCL, and
(3) illustrate the effect of a small number of dilution steps.

In the latter application, we saw a dramatic increase in the standard error of the estimate when the
variance approximation was introduced, potentially leading to a change of significance for the presented dataset depending on the desired significance level.
This illustrates that it is important to consider all aspects of uncertainty when conducting AE correction of qPCR experiments.
Problems with uncertainty in efficiency estimates should be handled by establishing well-estimated dilution curves as argued elsewhere \citep{Bustin2010}, however even in this case the presented method also allows for design guidelines for power calculations and assessing the influence of the estimated dilution curves.
It is also noteworthy that model based estimation of the $\ddcq$ also allows for model checking by e.g.\ residual plots.

Lastly, we note that the algorithm \citep{Mx300P2013} we used for threshold selection and $C_q$-value extraction in the CIC and DLBCL studies may not be optimal, cf.~\citet{Ruijter2013} and improvements by \citet{Spiess2015}, as it can be affected by the AE.
Nonetheless, this has no bearing on the stated problem of this paper.
The estimated standard error of $\ddcq$ is still affected in a similar manner by the uncertainty of the AE and thus too optimistic.

% Conclusions
Despite the extensive use of qPCR, more statistical research is needed to establish qPCR more firmly as a gold standard to reliably quantify abundances of nucleic acids.
Researchers analyzing qPCR experiments need to model their experiments in detail, e.g.\ via linear or non-linear (mixed) models, as the propagation of uncertainty needs to be carefully assessed and accounted for.
This is necessary for making valid inferences and upholding the common statistical guarantees often erroneously assumed to be automatically fulfilled.
We recommend the conservative and proper approach of \emph{always} accounting for the uncertainty of the AE.

\phantomsection
\addcontentsline{toc}{section}{Supplementary Material and Software}
\section*{Supplementary Material and Software}

The statistical analysis were done using the programming language \textsf{R} \texttt{v\Sexpr{paste0(version$major, ".", version$minor)}} \citep{Rproj} using \texttt{lme4}.
All data, R code, LaTeX, and instructions to reproduce the present paper and results are freely available at
\href{http://github.org/AEBilgrau/effadj/}{\texttt{github.org/AEBilgrau/effadj/}}
using \texttt{knitr}, an extension of Sweave \citep{Xie2013, Leisch2002}.
Functionality from the packages
\texttt{Hmisc} \citep{Harrell2015},
\texttt{lattice} (and \texttt{latticeExtra}) \citep{Sarkar2008},
\texttt{epiR} \citep{Stevenson2015},
\texttt{snowfall} \citep{Knaus2013}, and
\texttt{GMCM} \citep{Bilgrau2015},
were used for tables, figures, FDR/TPR confidence intervals, parallel execution of simulations, and multivariate normal simulations, respectively.

\phantomsection
\addcontentsline{toc}{section}{Acknowledgments}
\section*{Acknowledgments}
This work was supported by
the Myeloma Stem Cell Network funded by EU FP6; Karen Elise Jensen Fonden; and CHEPRE funded by the Danish Agency for Science, Technology and Innovation.
The founders had no role in study design, data collection, analysis, publishing, or preparation of this paper.
Poul Svante Eriksen is also thanked for his comments on the statistical methods.
 {\it Conflict of Interest}: None declared.


\phantomsection
\addcontentsline{toc}{section}{References}
\bibliographystyle{plainnat}
\bibliography{references}

\newpage

% CIC fig
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{fig1}
\end{center}
\caption{
  Overview of CIC experiment data.
  A: Raw $C_q$-values for different cell lines (samples) for each gene type and sample type.
  The point type and colour differentiates the different gene types.
  B: Dilution data for
  reference genes (\textit{ACTB}, \textit{GAPDH}) and
  target genes (\textit{MGST1}, \textit{MMSET}).
}
\label{fig:cqCIC}
\end{figure}

% CIC table
\input{../output/Table1.tex}


% DLBCL fig
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{fig2}
\end{center}
\caption{
  Overview of DLBCL testis experiment data.
  A: Raw $C_q$-values for different patient samples for each gene type and sample type.
  The point type and colour differentiates the different gene types.
  B: Dilution data for
  reference genes (\textit{RNU-24}, \textit{RNU-6B}) and
  target genes (\textit{miR-127}, \textit{miR-143}).
}
\label{fig:cqTestis}
\end{figure}

% DLBCL table
\input{../output/Table2.tex}



% Arabidopsis thaliana fig
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{fig3}
\end{center}
\caption{
  Overview of \citet{Yuan2008} experiment data.
  $C_q$-values against the dilution step for case and control samples.
  Dilution data are present for both the target (\textit{MT7}) and reference genes (Tublin, \textit{UBQ}).
  }
\label{fig:cqYuan}
\end{figure}

% Arabidopsis thaliana table
\input{../output/Table3.tex}


% Simulation tab
\input{../output/Table4.tex}

% Simulation fig
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{fig4}
\end{center}
\caption{
  Plot of the false positive rates (FPR, black) and true positive rates (TPR, grey) and their 95 \% confidence intervals achieved simulation experiments for each method at various $p$-value cut-offs (0.05, 0.01, 0.1) shown by solid red horizontal lines.
  The FPR and TPR are computed completely analogous to Table~\ref{tab:simexample}.
  The rates are plotted for each combination of 4 or 8 samples with 4 or 8 fold dilution curves.
}
\label{fig:simstudy}
\end{figure}


% Simulation fig2
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{fig5}
\end{center}
\caption{
  The mean standard error of the $\ddcq$ for two methods (EC and EC\&VA1) over \Sexpr{n.sims} repeated simulations under the null (panel A) and alternative hypothesis (panel B) as a function of the number of dilution steps for a different number of samples in each group.
}
\label{fig:simstudy2}
\end{figure}




\end{document}

